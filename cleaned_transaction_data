import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np

# Load the dataset
df = pd.read_csv("/Users/gillianmondero/Downloads/Transaction_Data.csv")

# Check the dataset structure
print(df.head())
print(df.info())

# Find and remove duplicate rows
duplicates = df[df.duplicated()]
print(f"Number of duplicate rows: {duplicates.shape[0]}")

df_before = df.shape
df = df.drop_duplicates()
df_after = df.shape

print(f"Shape before removing duplicates: {df_before}")
print(f"Shape after removing duplicates: {df_after}")

# Handle missing values
missing_values = df.isnull().sum()
print("Missing values in each column:\n", missing_values)

for column in df.select_dtypes(include=['float64', 'int64']).columns:
    if column == 'Transaction_Amount':  
        mean_value = round(df[column].mean(), 2)
        df[column] = df[column].fillna(mean_value)
    else:
        df[column] = df[column].fillna(df[column].mean())

# Confirm no missing values remain
missing_values_after = df.isnull().sum()
print("Missing values after imputation:\n", missing_values_after)

# Normalize Transaction_Amount
scaler = MinMaxScaler()
df['Normalized_Transaction_Amount'] = scaler.fit_transform(df[['Transaction_Amount']])

# One-hot encode Transaction_Type
df = pd.get_dummies(df, columns=['Transaction_Type'], prefix='Type')

# Convert Transaction_Date to datetime
df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'])

# Drop Transaction_ID since it's not needed anymore
df = df.drop(columns=['Transaction_ID'])

# Sort by Customer_ID and reset index
df_sorted = df.sort_values(by='Customer_ID', ascending=True)
df_sorted.reset_index(drop=True, inplace=True)

# Correlation matrix for the dataset
correlation_matrix = df.corr()
print("Correlation Matrix:\n", correlation_matrix)

# Handle outliers in Transaction_Amount using IQR
Q1 = df['Transaction_Amount'].quantile(0.25)
Q3 = df['Transaction_Amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['Transaction_Amount'] < lower_bound) | (df['Transaction_Amount'] > upper_bound)]
print(f"Outlier count: {outliers.shape[0]} ({(outliers.shape[0] / df.shape[0]) * 100:.2f}%)")

df['Transaction_Amount'] = df['Transaction_Amount'].clip(lower=lower_bound, upper=upper_bound)

# Double-check that no outliers remain
remaining_outliers = df[(df['Transaction_Amount'] < lower_bound) | (df['Transaction_Amount'] > upper_bound)].shape[0]
print(f"Remaining outliers: {remaining_outliers}")

# Aggregate features per Customer_ID
customer_features = df.groupby('Customer_ID').agg(
    Transactions_Per_Customer=('Customer_ID', 'size'),
    Average_Transaction_Amount=('Transaction_Amount', 'mean'),
    Total_Transaction_Amount=('Transaction_Amount', 'sum'),
    Recency=('Transaction_Date', lambda x: (df['Transaction_Date'].max() - x.max()).days),
    **{f'Count_{col}': (col, 'sum') for col in df.columns if col.startswith('Type_')}
).reset_index()

# Round monetary columns
customer_features['Average_Transaction_Amount'] = customer_features['Average_Transaction_Amount'].round(2)
customer_features['Total_Transaction_Amount'] = customer_features['Total_Transaction_Amount'].round(2)

# Normalize features for analysis or clustering
scaler = StandardScaler()
scaled_features = scaler.fit_transform(customer_features[['Transactions_Per_Customer', 
                                                          'Average_Transaction_Amount', 
                                                          'Total_Transaction_Amount', 
                                                          'Recency'] + [f'Count_{col}' for col in df.columns if col.startswith('Type_')]])

# Print cleaned customer features
print("Cleaned Customer Features:\n", customer_features)

# Ensure Customer_ID is sorted 
customer_features = customer_features.sort_values(by='Customer_ID')
customer_features.to_csv("customer_features.csv", index=False)
